{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Prerequisites\n",
    "- [Intro to PyTorch](https://github.com/AI-Core/Neural-Networks/blob/master/Intro%20to%20PyTorch.ipynb)\n",
    "\n",
    "Most tasks can be either defined as regression (predicting a continuous value e.g. house price) or classification (predicting a discrete value e.g. cat vs dog in images) problems.\n",
    "\n",
    "The simplest form of a classification problem is binary classification, where an example either belongs to a class or doesn't and there is only one class. That is, every example has a lable which is either true or false.\n",
    "\n",
    "In classification, we can interpret the output as a confidence (probability) that the example belongs to that class. Because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = #sample from a normal distribution\n",
    "    Y = # return binary vector with true where X above some threshold and false if below\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', marker='x')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build our binary classifier model class\n",
    "\n",
    "The model is going to be almost identical to the model that we've used for linear regression, except from the fact that the output will need be transformed using the sigmoid function so that it lies within the range 0-1 to represent a confidence of the example belonging to the positive class. We can find a PyTorch implementation of the sigmoid function [here](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid).\n",
    "\n",
    "![](./images/sigmoid.jpg)\n",
    "\n",
    "Let's implement our classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #define the linear transformation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply linear transformation\n",
    "        #add a sigmoid activation function to the end\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function for binary classification\n",
    "\n",
    "Unlike in regression problems, how we measure the loss for a particular example in a classification problem depends on what the label is. That is, if the label is 1 (yes, it belongs to the positive class) then we want to penalise it more for having a prediction nearer to 0 (no, it belongs to the negative class), and vice versa.\n",
    "\n",
    "### Binary cross entropy\n",
    "\n",
    "![](./images/bce.jpg)\n",
    "[Cross entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, Y =  #convert data to tensors and add batch dimension\n",
    "Y = Y.double() #convert Y to a double\n",
    "\n",
    "H = Classifier().double() # initialise classifier and convert to double\n",
    "optimiser = torch.optim.Adam(H.parameters(), lr=0.01)# create optimiser\n",
    "\n",
    "# PLOT OUR HYPOTHESIS BEFORE TRAINING\n",
    "plt.figure()\n",
    "plt.title('Before training')\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('Features')\n",
    "plt.scatter(X, H(X).detach(), label='predictions')\n",
    "plt.scatter(X, Y, c='r', marker='x', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for x, y in zip(X, Y):\n",
    "        # make prediction\n",
    "        # compute loss\n",
    "        # calculate derivatives\n",
    "        # make optimisation step\n",
    "        # reset gradients\n",
    "        epoch_losses.append(loss)\n",
    "    losses.append(sum(epoch_losses)/len(epoch_losses))\n",
    "        \n",
    "# PLOT THE LOSS CURVE AND OUR HYPOTHESIS AFTER TRAINING\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title('Loss curve')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.plot(losses)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_title('After Training')\n",
    "ax2.set_ylabel('Label')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.scatter(X, H(X).detach(), label='predictions')\n",
    "ax2.scatter(X, Y, c='r', marker='x', label='ground truth')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification vs multiclass classification\n",
    "\n",
    "In binary classification the output must be either true or false. Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents a confidence in the fact that the example belongs to the positive class. Alternatively, still for binary classification, we could have two output nodes, where the value of the first represents the confidence that the input belongs to the positive class (true/class 1) and the value of the second represents the confidence that the input belongs to the negative class (false/class 2). In this case, the values of each output node must be positive and they must sum to 1, because this output layer represents a probability distribution over the output classes. \n",
    "\n",
    "![](./images/binary-class.jpg)\n",
    "\n",
    "In the case where we have two nodes to represent true and false, we can think about it as having trained two models, which have exactly the same weights in every layer except for the last. \n",
    "\n",
    "Treating true and false as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "![](./images/multiclass.jpg)\n",
    "\n",
    "### What function can we use to convert the output layer into a distribution over classes?\n",
    "\n",
    "The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "![](./images/softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You've finished this notebook\n",
    "\n",
    "Next steps:\n",
    "- [Tensorboard](https://github.com/AI-Core/Neural-Networks/blob/master/Tensorboard.ipynb)\n",
    "- [Neural networks](https://github.com/AI-Core/Neural-Networks/blob/master/Neural%20Networks.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
