{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to machine learning\n",
    "\n",
    "## Prerequisites\n",
    "- [Basic Python](https://github.com/AI-Core/Python)\n",
    "\n",
    "## What is machine learning, and why do we care about it?\n",
    "\n",
    "### The intelligence perspective\n",
    "\n",
    "Through experience, humans learn to act intelligently - to acquire and apply knowledge. We are all able to learn to accomplish tasks for ourselves, without explicitly being told how. However, there are still lots of important tasks that we don't know how to accomplish - maybe because we haven't had enough time to work on them, or maybe because. Machine learning is the field which focuses on how to have machines learn to accomplish goals for themselves.\n",
    "\n",
    "In general, the problem of accomplishing goals can be framed as trying to produce some intended output given an input. As such, machine learning is all about automatically learning to represent the relationships between inputs and outputs. These inputs and outputs can take many forms as illustrated below.\n",
    "\n",
    "![](images/inp-out.jpg)\n",
    "\n",
    "We'd like to be able to build algorithms that can learn to use inputs to predict useful outputs, and solve problems such as those shown above.\n",
    "\n",
    "**Building machines which can model the world (make accurate predictions about it) is the goal of AI.**\n",
    "\n",
    "### The modelling perspective\n",
    "\n",
    "If we had a model of the universe (like a perfect simulation) then we'd be able to solve all kinds of problems.\n",
    "We'd be able to simulate what the weather would be like tomorrow. \n",
    "Or what drugs will be effective for treating diseases.\n",
    "Or what combination of words will make someone do exactly what you want.\n",
    "Or all other problems.\n",
    "\n",
    "However, we actually never have a perfect model of the world - it's just too complex!\n",
    "\n",
    "\"All models are wrong, but some are useful\" - George Box\n",
    "\n",
    "Because we can't have perfect models, we have to settle with predictive models. \n",
    "\n",
    "In order to achieve goals, animals have to understand how the world works. \n",
    "Over time they build up mental models of how the world behaves. \n",
    "If I see clouds I can predict how likely it is to rain.\n",
    "If I push this chair i can predict whether it will slide across the floor or topple over.\n",
    "For these simple examples, we can make pretty good predictions.\n",
    "This is probably because we have experienced a lot of similar situations, and can generalise\n",
    "\n",
    "## How can we approach automatically learning to model the relationships between inputs and outputs?\n",
    "\n",
    "Almost all machine learning algorithms consist of 4 components:\n",
    "1. the data\n",
    "2. the model\n",
    "3. the criterion\n",
    "4. the optimiser\n",
    "\n",
    "This notebook will introduce you to all of those, with simple, practical examples.\n",
    "\n",
    "## Side note - three types of Machine Learning\n",
    "\n",
    "There are three main categories of problems within Machine Learning. It's worth knowing these straight away, although for this series we will only consider *supervised learning*.\n",
    "\n",
    "### **Unsupervised learning** \n",
    "Where we only have an input and try to predict something useful as an output, without being explicitly shown examples of what the output should be. This what data is likely to in order to better understand the underlying structure of it. E.g. we have data about houses and try to split these examples into different clusters.\n",
    "\n",
    "### **Reinforcement Learning**\n",
    "Where our algorithm controls and agent that interacts with it's evironment and has to learn what actions to take to perform a task. E.g. we are trying to get an robot to walk or an algorithm to learn how to win a game of chess.\n",
    "\n",
    "### **Supervised Learning**\n",
    "Where we predict an output from a input, given examples of input-output pairs. E.g. we use different features about a house such as location, number of rooms, etc and try to predict the price.\n",
    "\n",
    "Common synonyms\n",
    "- Loss funtion = cost function = criterion = error function\n",
    "- Inputs = Features\n",
    "- Outputs = Labels\n",
    "\n",
    "In this notebook, and the first series of lectures, we will be learning about supervised learning - where we have datasets with both input features and output labels. Later notebooks will cover unsupervised learning and reinforcement learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data - What problem are we solving?\n",
    "\n",
    "As mentioned, in supervised learning we have both inputs and outputs for every example in a dataset.\n",
    "What the inputs and outputs in each example represent, fully determines the problem that you are trying to solve - whether it is image classification (input=image, output=label), language translation (input=spanish phrase, output=english phrase) or something totally different.\n",
    "\n",
    "Regardless of what the data represents, it must be expressed mathematically.\n",
    "\n",
    "![image](images/data.jpg)\n",
    "![image](images/labels.jpg)\n",
    "\n",
    "As shown above, our supervised dataset consists of $m$ inputs. Each of these inputs has $n$ features (n-dimensional feature vectors). And each input has a corresponding label (it's a supervised dataset).\n",
    "\n",
    "As we illustrated earlier, inputs and outputs can take many forms. The inputs do not have to be vectors and the labels do not have to be scalars as shown in the example.\n",
    "\n",
    "- Image inputs could be matrices with width and height, rather than just vector length.\n",
    "- Inputs could only have one feature, in which case they would be scalars (this is what we will implement now)\n",
    "\n",
    "- If we were trying to classify an input as a member of a particular, discrete class (classifying dogs vs cats vs turtles for example), then we would have a vector output with the length of the number of classes - where each of those elements represents the confidence with which our model predicts the input to be a member of that corresponding class.\n",
    "- Outputs could be an image or a video or a sound wave or spectrogram\n",
    "\n",
    "Lets create a function that generates some artificial data.\n",
    "\n",
    "The function should return an array of scalar features and an array of scalar labels which have a linear relationship (straight line, i.e. $y=wx + b$).\n",
    "Both the features and labels should have length $m$ - which is an argument to the function. \n",
    "Although data collected in the real world often has much more complex correlations, linear functions make simple  test cases to learn about machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0443849f100b>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0443849f100b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    X =  # m random values from a random normal distribution\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_linear_data(m=20): \n",
    "    ground_truth_w = 2.3 # slope (weight)\n",
    "    ground_truth_b = -8 # intercept (bias)\n",
    "    X =  # m random values from a random normal distribution\n",
    "    Y =  # compute the output (with some random noise added)\n",
    "    return X, Y # returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure() # create a figure\n",
    "    plt.scatter(X, Y, c='r') # plot the data in color=red\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "    \n",
    "m = 10 # number of examples\n",
    "X, Y = sample_linear_data(m)\n",
    "print('X:',X, '\\n')\n",
    "print('Y:',Y, '\\n')\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The model - How can we make predictions?\n",
    "We want to be able use information that we have, to predict useful information that we don't have. \n",
    "This is an input-output problem - we have an input (info we know), we want an output (info we don't have).\n",
    "Mathematical functions can represent input output relationships e.g. $y = 2x +3$.\n",
    "So we will use mathematical functions to model the relationship between our inputs and outputs.\n",
    "Our goal is to model the world, and use that to be able to intelligently infer a lot from a little.\n",
    "\n",
    "Lets create a simple model that represents a straight line (linear) relationship between the input and output and use it to make predictions about outputs, given inputs. It should be able to take in an array of inputs form different examples and output predictions for all of them in parallel. \n",
    "\n",
    "Our model will be of the form $y = wx + b$.\n",
    "\n",
    "![title](images/NN1_singlevar_lr_equation.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-fbbeb5acf999>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-fbbeb5acf999>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    self.w =  # randomly initialise weight\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): #initalize parameters \n",
    "        self.w =  # randomly initialise weight\n",
    "        self.b =  # randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        ypred = # make a prediction\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = # set this instance's weights to the new weight value passed to the function\n",
    "        self.b = # do the same for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = # instantiate our linear model\n",
    "y_hat = # make prediction\n",
    "print('Input:',X, '\\n')\n",
    "print('W:', H.w, 'B:', H.b, '\\n')\n",
    "print('Prediction:', y_hat, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise our hypothesis against the true features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The criterion - How do we know how good our model is?\n",
    "\n",
    "#### The criterion will also be referred to as the loss function, cost function, error function or objective.\n",
    "\n",
    "Our criterion should be a measure of how bad our model is. We will use it to compare different models. As the model gets worse the loss function should return larger values.\n",
    "\n",
    "### Mean squared error (MSE) loss\n",
    "\n",
    "**One way** to evaluate the performance of a model that predicts continuous (not discrete or bounded) outputs is to use the mean squared error loss. This does exactly what you think: it calculates the error (difference between our model's prediction and the true label) and then squares it and takes the mean of those square errors for each example. Squaring any value makes it positive, so as long as the error is not zero it will increase the value of the loss - regardless of whether our prediction is below (negative error) or above (positive error) the value of the label, the values of that **squared** difference will increase the returned loss.\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. cross entropy loss for classification)\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function. It should take in an array of predictions for different example inputs as well as an array of corresponding example labels. It should return a single number (scalar) that represents the MSE loss. \n",
    "\n",
    "![title](images/NN1_cost_function.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(y_hat, labels):\n",
    "    errors = # calculate errors\n",
    "    squared_errors = # square errors\n",
    "    mean_squared_error = # calculate mean \n",
    "    return mean_squared_error # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = L(y_hat, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The optimiser\n",
    "\n",
    "The optimiser optimises our model. Most machine learning models are **parametric**, which means that the function which they represent depends on their parameters (in our case the weight (slope) and bias (intercept)). Different optimisers improve our models using different algorithms.\n",
    "\n",
    "In this notebook we will implement some fundamental optimisation techniques: random search and grid search.\n",
    "\n",
    "### Random Search\n",
    "Random seach is the process of randomly choosing values within a specified range and testing them to evaluate how good they are. E.g. test random values between 0 and 10.\n",
    "\n",
    "![](images/NN1_randomsearch.JPG)\n",
    "\n",
    "Let's implement a function that tries a bunch of possible values for the weight and bias of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n_samples, limit=7):\n",
    "    best_weights = # no best weight found yet\n",
    "    best_bias = # no best bias found yet\n",
    "    lowest_cost = # initialize it very high\n",
    "    for i in  # try this many different parameterisations\n",
    "        w = np.random.uniform(-limit, limit) # randomly sample a weight within the limits of the search\n",
    "        b = np.random.uniform(-limit, limit) # randomly sample a bias within the limits of the search\n",
    "        H. # update our model with random parameters\n",
    "        y_hat = # make prediction\n",
    "        cost =  # calculate loss\n",
    "        if  # if this is the best parameterisation so far\n",
    "            lowest_cost =  # update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights = # get best weights so far from the model\n",
    "            best_bias = # get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights, best_bias = random_search(100) # do 100 samples in a random search \n",
    "H. # make sure to set our model's weights to the best values we found\n",
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "Our random search optimisation was able to fit the input-output relationship of our data! Or at least it got close. \n",
    "\n",
    "#### Why doesn't it get closer as we sample more potential parameterisations? \n",
    "This is because of the limits of the values of the parameters that we perform the grid search over. In this case, by default we are only trying parameters in the range from -7 to 7. But the true bias is -8 which is outside of this range. So we have made a mistake in assuming the range of values that our optimal parameterisation might be included in. Feel free to change this limit in the function definition to see the model converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Grid search is the process of trying out values at common intervals within a specified range for each parameter, and testing them to evaluate how good they are. E.g. test the values [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "![](images/NN1_gridsearch.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "def generate_grid_search_values(n_params, n_samples_per_param=10, minval=-2.5, maxval=2.5):\n",
    "    print(f'Trying {n_samples_per_param} samples per parameter')\n",
    "    param_values =  # get list of different values to try for each parameter (use minval and maxval arguments) \n",
    "    grid_samples =  # get a list of different (weight, bias) pairs to try (look up the permutations function we imported)\n",
    "    return grid_samples\n",
    "\n",
    "def grid_search(grid_search_values):\n",
    "    best_weights =  # no best weight found yet\n",
    "    best_bias =  # no best bias found yet\n",
    "    lowest_cost =  # initialize it very high\n",
    "    for search_val in grid_search_values: # for each model parameterisation that we will try \n",
    "        # update model parameters\n",
    "        y_hat = # make prediction\n",
    "        cost = # calculate loss\n",
    "        if  # if this is the best parameterisation so far\n",
    "            lowest_cost =  # update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights =  # get best weights so far from the model\n",
    "            best_bias =  # get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_values = generate_grid_search_values(2, n_samples_per_param=100) # generate model parameterisations to try\n",
    "best_weights, best_bias = grid_search(grid_search_values) # perform grid search\n",
    "H. # update model with best parameters found\n",
    "plot_h_vs_y(X, H(X), Y) # plot predictions and true labels\n",
    "\n",
    "grid_search_values = generate_grid_search_values(2, n_samples_per_param=99) # generate model parameterisations to try\n",
    "best_weights, best_bias = grid_search(grid_search_values) # perform grid search\n",
    "H.update_params(best_weights, best_bias) # update model with best parameters found\n",
    "plot_h_vs_y(X, H(X), Y) # plot predictions and true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "#### Why the difference in predictions from 100 samples (pretty good) and 99 samples (pretty bad)? \n",
    "\n",
    "Well, grid search will only test parameterisations that are exactly on it's grid. In the case of the 99 samples, the optimal parameters do exist within this region, but they dont lie exactly on the grid - they lie between points that we test on the grid. Hence those optimal parameters are not found. \n",
    "\n",
    "With the 100 samples, one of the parameterisations (pairs of weight and bias) on our grid lies close to the actual optimal values. So the model manages to much better represent the input-output relationship between our features and labels.\n",
    "\n",
    "Another danger would be that the space which we are searching with our grid does not contain the optimal parameterisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will random search and grid search get us all the way?\n",
    "\n",
    "Aside from the issues showcased above, the major limitation of these search methods is how they scale with the number of parameters in our model. To model more complex functions we'll need more complex models - models with more parameters (the models we'll use next workshop will have at least thousands). But the time taken for these search methods scales **exponentially** with the number of parameters. This is because these methods have to search the whole space, and they keep searching even if they find the optimal value (they can't be sure it's the best parameterisation in the domain that they're checking until they've compared it to everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yes, you made it!\n",
    "In this notebook, we learnt the very basic recipe for making machine learning algorithms. This consisted of:\n",
    "1. The data - our examples of inputs and outputs (in the supervised case) which determine the function that our model will learn to represent and hence the problem that we are solving\n",
    "2. The model - our mathematical function that we pass our data forward through to make a prediction for the output\n",
    "3. The criterion - how we measure how bad our model is. We used the mean squared error loss function.\n",
    "4. The optimiser - our method for updating the parameters of our models. We tried out random search and grid search.\n",
    "\n",
    "\n",
    "## Next steps\n",
    "\n",
    "- [Gradient based optimisation]() - in this notebook we will look at optimisation techniques that do scale to more complex models and problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
