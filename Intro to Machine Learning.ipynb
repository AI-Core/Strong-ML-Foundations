{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to machine learning\n",
    "\n",
    "## Prerequisites\n",
    "- [Basic Python](https://github.com/AI-Core/Python/blob/master/Intro%20to%20Python%20solutions.ipynb)\n",
    "\n",
    "## Why do we care about machine learning?\n",
    "\n",
    "If we had a model of the world (like a perfect simulation) then we'd be able to solve all problems.\n",
    "We'd be able to simulate what the weather would be like tomorrow. \n",
    "Or what drugs will be effective for treating diseases.\n",
    "Or what combination of words will make someone do exactly what you want.\n",
    "Or all other problems.\n",
    "\n",
    "**Building machines which can model the world (make accurate predictions about it) is the goal of AI.**\n",
    "\n",
    "However, we actually never have a perfect model of the world - the world is too complex!\n",
    "\n",
    "\"All models are wrong, but some are useful\" - George Box\n",
    "\n",
    "Because we can't have perfect models, we have to settle with predictive models. \n",
    "\n",
    "In order to achieve goals, animals have to understand how the world works. \n",
    "Over time they build up mental models of how the world behaves. \n",
    "If I see clouds I can predict how likely it is to rain.\n",
    "If I push this chair i can predict whether it will slide across the floor or topple over.\n",
    "For these simple examples, we can make pretty good predictions.\n",
    "This is probably because we have experienced a lot of similar situations, and can generalise\n",
    "\n",
    "Machine learning is all about learning to represent the relationships between inputs and outputs. These inputs and outputs can take many forms.\n",
    "\n",
    "![](images/inp-out.jpg)\n",
    "\n",
    "We'd like to be able to build algorithms that can learn to use inputs to predict useful outputs.\n",
    "\n",
    "Almost all machine learning algorithms consist of 4 components:\n",
    "1. the data\n",
    "2. the model\n",
    "3. the criterion\n",
    "4. the optimiser\n",
    "\n",
    "This notebook will introduce you to all of those, with simple, practical examples.\n",
    "\n",
    "## Side note - three types of Machine Learning\n",
    "\n",
    "There are three main categories of problems within Machine Learning. It's worth knowing these straight away.\n",
    "\n",
    "### **Unsupervised learning** \n",
    "Where we only have an input and try to predict something useful as an output, without being explicitly shown examples of what the output should be. This  what data is likely to in order to better understand the underlying structure of it. E.g. we have data about houses and try to split these examples into different clusters.\n",
    "\n",
    "### **Reinforcement Learning**\n",
    "Where our algorithm interacts with it's evironment and has to learn what actions to take to perform a task. E.g. we are trying to get an robot to walk or an algorithm to learn how to win at chess.\n",
    "\n",
    "### **Supervised Learning**\n",
    "Where we predict an output from a input, given examples of input-output pairs. E.g. we use different features about a house such as location, number of rooms, etc and try to predict the price.\n",
    "\n",
    "Common synonyms\n",
    "- Loss funtion = cost function = criterion = error function\n",
    "- Inputs = Features\n",
    "- Outputs = Labels\n",
    "\n",
    "In this notebook, and the first series of lectures, we will be learning about supervised learning - where we have datasets with both input features and output labels. Later notebooks will cover unsupervised learning and reinforcement learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data - What problem are we solving?\n",
    "Lets create a function that generates some artificial data.\n",
    "\n",
    "The function should return an array of features and an array of labels which have a linear relationship (straight line, i.e. $y=wx + b$).\n",
    "Both the features and labels should have length $m$ - which is an argument to the function. \n",
    "Although data collected in the real world often has much more complex correlations, linear functions make simple  test cases to learn about machine learning algorithms.\n",
    "\n",
    "![image](images/data.jpg)\n",
    "![image](images/labels.jpg)\n",
    "\n",
    "\n",
    "As shown above, our supervised dataset consists of $m$ inputs. Each of these inputs has $n$ features (n-dimensional feature vectors). And each input has a corresponding label (it's a supervised dataset).\n",
    "\n",
    "As we illustrated earlier, inputs and outputs can take many forms. The inputs do not have to be vectors and the labels do not have to be scalars as shown in the example.\n",
    "\n",
    "- Image inputs could be matrices with width and height, rather than just vector length.\n",
    "- Inputs could only have one feature, in which case they would be scalars (this is what we will implement now)\n",
    "\n",
    "- If we were trying to classify an input as a member of a particular, discrete class (classifying dogs vs cats vs turtles for example), then we would have a vector output with the length of the number of classes - where each of those elements represents the confidence with which our model predicts the input to be a member of that corresponding class.\n",
    "- Outputs could be an image or a video or a sound wave or spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_linear_data(m=20): \n",
    "    ground_truth_w = 2.3 # slope (weight)\n",
    "    ground_truth_b = -8 # intercept (bias)\n",
    "    X =  # m random values from a random normal distribution\n",
    "    Y =  # compute the output (with some random noise added)\n",
    "    return X, Y # returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure() # create a figure\n",
    "    plt.scatter(X, Y, c='r') # plot the data in color=red\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "    \n",
    "m = 10\n",
    "X, Y = sample_linear_data(m)\n",
    "print('X:',X, '\\n')\n",
    "print('Y:',Y, '\\n')\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The model - How can we make predictions?\n",
    "We want to be able use information that we have, to predict useful information that we don't have. \n",
    "This is an input output problem - we have an input (info we know), we want an output (info we don't have).\n",
    "Mathematical functions can represent input output relationships e.g. $y = 2x +3$.\n",
    "So we will use mathematical functions to model the relationship between our inputs and outputs.\n",
    "Our goal is to model the world, and use that to be able to intelligently infer a lot from a little.\n",
    "\n",
    "Lets create a simple model that represents a straight line (linear) relationship between the input and output and use it to make predictions about outputs, given inputs.\n",
    "\n",
    "Our model will be of the form $y = wx + b$.\n",
    "\n",
    "![title](images/NN1_singlevar_lr_equation.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): #initalize parameters \n",
    "        self.w =  # randomly initialise weight\n",
    "        self.b =  # randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        ypred = # make a prediction\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = # set this instance's weights to the new weight value passed to the function\n",
    "        self.b = # do the same for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = # instantiate our linear model\n",
    "y_hat = # make prediction\n",
    "print('Input:',X, '\\n')\n",
    "print('W:', H.w, 'B:', H.b, '\\n')\n",
    "print('Prediction:', y_hat, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise our hypothesis against the true features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The criterion - How do we know how good our model is?\n",
    "\n",
    "#### The criterion will also be referred to as the loss function, cost function, error function.\n",
    "\n",
    "Our criterion should be a measure of how bad our model is. We will use it to compare different models. As the model gets worse the loss function should return larger values.\n",
    "\n",
    "### Mean squared error (MSE) loss\n",
    "\n",
    "**One way** to evaluate the performance of a model that predicts continuous (not discrete or bounded) outputs is to use the mean squared error loss. This does exactly what you think: it calculates the error (difference between our model's prediction and the true label) and then squares it and takes the mean of those square errors for each example. Squaring any value makes it positive, so as long as the error is not zero it will increase the value of the loss - regardless of whether our prediction is below or above the value of the label, the values of that **squared** difference will increase the returned loss.\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. cross entropy loss for classification)\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function.\n",
    "\n",
    "![title](images/NN1_cost_function.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(y_hat, labels):\n",
    "    errors = # calculate errors\n",
    "    squared_errors = # square errors\n",
    "    mean_squared_error = # calculate mean \n",
    "    return mean_squared_error # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = L(y_hat, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The optimiser\n",
    "\n",
    "The optimiser optimises our model. Most machine learning models are **parametric**, which means that the function which they represent depends on their parameters (in our case the weight (slope) and bias (intercept)). Different optimisers improve our models using different algorithms.\n",
    "\n",
    "In this notebook we will implement some fundamental optimisation techniques: random search and grid search.\n",
    "\n",
    "### Random Search\n",
    "Random seach is the process of randomly choosing values within a specified range and testing them to evaluate how good they are. E.g. test random values between 0 and 10.\n",
    "\n",
    "![](images/NN1_randomsearch.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n_samples, limit=7):\n",
    "    best_weights = # no best weight found yet\n",
    "    best_bias = # no best bias found yet\n",
    "    lowest_cost = # initialize it very high\n",
    "    for i in  # try this many different parameterisations\n",
    "        w = np.random.uniform(-limit, limit) # randomly sample a weight within the limits of the search\n",
    "        b = np.random.uniform(-limit, limit) # randomly sample a bias within the limits of the search\n",
    "        H. # update our model with random parameters\n",
    "        y_hat = # make prediction\n",
    "        cost =  # calculate loss\n",
    "        if  # if this is the best parameterisation so far\n",
    "            lowest_cost =  # update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights = # get best weights so far from the model\n",
    "            best_bias = # get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights, best_bias = random_search(100) # do 100 samples in a random search \n",
    "H. # make sure to set our model's weights to the best values we found\n",
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "Our random search optimisation was able to fit the input-output relationship of our data! Or at least it got close. \n",
    "\n",
    "#### Why doesn't it get closer as we sample more potential parameterisations? \n",
    "This is because of the limits of the values of the parameters that we perform the grid search over. In this case, by default we are only trying parameters in the range from -7 to 7. But the true bias is -8 which is outside of this range. So we have made a mistake in assuming the range of values that our optimal parameterisation might be included in. Feel free to change this limit in the function definition to see the model converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Grid search is the process of trying out values at common intervals within a specified range and testing them to evaluate how good they are. E.g. test the values [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "![](images/NN1_gridsearch.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "def generate_grid_search_values(n_params, n_samples=100, minval=-2.5, maxval=2.5):\n",
    "    n_samples_per_param = int(np.power(n_samples, 1 / n_params)) # want 100 samples for 2 variables, so try 10 different values for each parameter because 10^2=100\n",
    "    print(f'Trying {n_samples_per_param} samples per parameter')\n",
    "    param_values =  # get list of different parameters to try\n",
    "    grid_samples =  # try every possible permutation of the param values\n",
    "    return grid_samples\n",
    "\n",
    "def grid_search(grid_search_values):\n",
    "    best_weights =  # no best weight found yet\n",
    "    best_bias =  # no best bias found yet\n",
    "    lowest_cost =  # initialize it very high\n",
    "    for search_val in grid_search_values: # for each model parameterisation that we will try \n",
    "        # update model parameters\n",
    "        y_hat = # make prediction\n",
    "        cost = # calculate loss\n",
    "        if  # if this is the best parameterisation so far\n",
    "            lowest_cost =  # update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights =  # get best weights so far from the model\n",
    "            best_bias =  # get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try using 100 grid search values\n",
    "grid_search_values = generate_grid_search_values(2, n_samples=100) # generate model parameterisations to try\n",
    "best_weights, best_bias = grid_search(grid_search_values) # perform grid search\n",
    "H. # update model with best parameters found\n",
    "plot_h_vs_y(X, H(X), Y) # plot predictions and true labels\n",
    "\n",
    "grid_search_values = generate_grid_search_values(2, n_samples=99) # generate model parameterisations to try\n",
    "best_weights, best_bias = grid_search(grid_search_values) # perform grid search\n",
    "H.update_params(best_weights, best_bias) # update model with best parameters found\n",
    "plot_h_vs_y(X, H(X), Y) # plot predictions and true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "#### Why the difference in predictions from 100 samples (pretty good) and 99 samples (pretty bad)? \n",
    "\n",
    "Well, grid search will only test parameterisations that are exactly on it's grid. In the case of the 99 samples, the optimal parameters do exist within this region, but they dont lie exactly on the grid - they lie between points that we test on the grid. Hence those optimal parameters are not found. \n",
    "\n",
    "With the 100 samples, one of the parameterisations (pairs of weight and bias) on our grid lies close to the actual optimal values. So the model manages to much better represent the input-output relationship between our features and labels.\n",
    "\n",
    "Another danger would be that the space which we are searching with our grid does not contain the optimal parameterisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will random search and grid search get us all the way?\n",
    "\n",
    "Aside from the issues showcased above, the major limitation of these search methods is how they scale with the number of parameters in our model. To model more complex functions we'll need more complex models - models with more parameters. But the time taken for these search methods scales **exponentially** with the number of parameters. This is because these methods have to search the whole space, and they keep searching even if they find the optimal value (they can't be sure it's the best parameterisation in the domain that they're checking until they've compared it to everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yes, you made it!\n",
    "In this notebook, we learnt the very basic recipe for making machine learning algorithms. This consisted of:\n",
    "1. The data - our examples of inputs and outputs (in the supervised case) which determine the function that our model will learn to represent and hence the problem that we are solving\n",
    "2. The model - our mathematical function that we pass our data forward through to make a prediction for the output\n",
    "3. The criterion - how we measure how bad our model is. We used the mean squared error loss function.\n",
    "4. The optimiser - our method for updating the parameters of our models. We tried out random search and grid search.\n",
    "\n",
    "\n",
    "## Next steps\n",
    "- [Gradient based optimisation](https://github.com/AI-Core/Strong-ML-Foundations/blob/master/Gradient%20based%20optimisation.ipynb) - in this notebook we will look at optimisation techniques that do scale to \n",
    "more complex models and problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
